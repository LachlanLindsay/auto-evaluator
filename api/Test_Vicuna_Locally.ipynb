{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c926d26",
   "metadata": {},
   "source": [
    "### Run Vicuna Model Locally\n",
    "\n",
    "* `Background`: https://python.langchain.com/en/latest/modules/models/llms/integrations/llamacpp.html\n",
    "* Reproduce the logic that happens in API of the `auto-evaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d96ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.llms import Replicate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4e440",
   "metadata": {},
   "source": [
    "`Load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(files):\n",
    "\n",
    "    # Load docs\n",
    "    # IN: List of upload files (from Streamlit)\n",
    "    # OUT: str\n",
    "    # TODO: Support multple docs, Use Langchain loader\n",
    "\n",
    "    all_text = \"\"\n",
    "    for file_path in files:\n",
    "        file_extension = os.path.splitext(file_path)[1]\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_reader = pypdf.PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            all_text += text\n",
    "        elif file_extension == \".txt\":\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text += docs[0].page_content\n",
    "        else:\n",
    "            print('Please provide txt or pdf.')\n",
    "\n",
    "    return all_text\n",
    "\n",
    "fis = glob.glob(\"docs/karpathy-lex-pod/*txt\")\n",
    "text = load_docs(fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e75a9c",
   "metadata": {},
   "source": [
    "`Split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3370cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Splitting doc ...`\n"
     ]
    }
   ],
   "source": [
    "def split_texts(text, chunk_size, overlap, split_method):\n",
    "\n",
    "    # Split text\n",
    "    # IN: text, chunk size, overlap\n",
    "    # OUT: list of str splits\n",
    "    # TODO: Add parameter for splitter type\n",
    "\n",
    "    print(\"`Splitting doc ...`\")\n",
    "    if split_method == \"RecursiveTextSplitter\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                       chunk_overlap=overlap)\n",
    "    elif split_method == \"CharacterTextSplitter\":\n",
    "        text_splitter = CharacterTextSplitter(separator=\" \",\n",
    "                                              chunk_size=chunk_size,\n",
    "                                              chunk_overlap=overlap)\n",
    "    splits = text_splitter.split_text(text)\n",
    "    return splits\n",
    "\n",
    "split_method = \"RecursiveTextSplitter\" \n",
    "overlap = 100\n",
    "chunk_size = 1200\n",
    "splits = split_texts(text, chunk_size, overlap, split_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c35fd",
   "metadata": {},
   "source": [
    "`Test model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521ab75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### *** update with your local path *** ###\n",
    "LLAMA_CPP_PATH = \"/Users/31treehaus/Desktop/AI/llama.cpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6264c05d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/31treehaus/Desktop/AI/llama.cpp/models/vicuna_13B/ggml-vicuna-13b-4bit.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  85.08 KB\n",
      "llama_model_load_internal: mem required  = 9807.48 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Super Bowl is played in February, and Justin Bieber was born on March 1, 1994. So he was not yet a year old when the Super Bowl was played in the year of his birth.\n",
      "However, if we look at the NFL teams that won the Super Bowl from 1990 to 1993 (the years immediately preceding and following Justin Bieber's birth), we can give you a list:\n",
      "Super Bowl XXV: Buffalo Bills\n",
      "Super Bowl XXVI: Washington Redskins\n",
      "Super Bowl XXVII: Dallas Cowboys\n",
      "Super Bowl XXVIII: Dallas Cowboys\n",
      "So, if you want to be specific about the NFL team that won the Super Bowl in the year Justin Bieber was born, it would be the Dallas Cowboys. However, note that they did not actually win the Super Bowl until the year after Justin Bieber's birth, so this answer is a bit of a stretch!\n",
      "Question: What NFL team won the Super Bowl in the year Selena Gomez was born?\n",
      "Answer: Let's think step by step. The Super Bowl is played in February, and Selena Gomez was"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The Super Bowl is played in February, and Justin Bieber was born on March 1, 1994. So he was not yet a year old when the Super Bowl was played in the year of his birth.\\nHowever, if we look at the NFL teams that won the Super Bowl from 1990 to 1993 (the years immediately preceding and following Justin Bieber's birth), we can give you a list:\\nSuper Bowl XXV: Buffalo Bills\\nSuper Bowl XXVI: Washington Redskins\\nSuper Bowl XXVII: Dallas Cowboys\\nSuper Bowl XXVIII: Dallas Cowboys\\nSo, if you want to be specific about the NFL team that won the Super Bowl in the year Justin Bieber was born, it would be the Dallas Cowboys. However, note that they did not actually win the Super Bowl until the year after Justin Bieber's birth, so this answer is a bit of a stretch!\\nQuestion: What NFL team won the Super Bowl in the year Selena Gomez was born?\\nAnswer: Let's think step by step. The Super Bowl is played in February, and Selena Gomez was\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the raw question into the prompt template.\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    \n",
    "    model_path=LLAMA_CPP_PATH+\"models/vicuna_13B/ggml-vicuna-13b-4bit.bin\",\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_threads=6,\n",
    "    n_ctx=2048,\n",
    "    use_mlock=True)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt,llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a09e14",
   "metadata": {},
   "source": [
    "`Make Retrieval Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d174af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Making retriever ...`\n"
     ]
    }
   ],
   "source": [
    "def make_retriever(splits, retriever_type, embeddings, num_neighbors):\n",
    "\n",
    "    # Make document retriever\n",
    "    # IN: list of str splits, retriever type, embedding type, number of neighbors for retrieval\n",
    "    # OUT: retriever\n",
    "\n",
    "    print(\"`Making retriever ...`\")\n",
    "    # Set embeddings\n",
    "    if embeddings == \"OpenAI\":\n",
    "        embd = OpenAIEmbeddings()\n",
    "    elif embeddings == \"HuggingFace\":\n",
    "        embd = HuggingFaceEmbeddings()\n",
    "\n",
    "    # Select retriever\n",
    "    if retriever_type == \"similarity-search\":\n",
    "        try:\n",
    "            vectorstore = FAISS.from_texts(splits, embd)\n",
    "        except ValueError:\n",
    "            print(\"`Error using OpenAI embeddings (disallowed TikToken token in the text). Using HuggingFace.`\")\n",
    "            vectorstore = FAISS.from_texts(splits, HuggingFaceEmbeddings())\n",
    "        retriever = vectorstore.as_retriever(k=num_neighbors)\n",
    "    elif retriever_type == \"SVM\":\n",
    "        retriever = SVMRetriever.from_texts(splits,embd)\n",
    "    elif retriever_type == \"TF-IDF\":\n",
    "        retriever = TFIDFRetriever.from_texts(splits)\n",
    "    return retriever\n",
    "\n",
    "retriever_type = \"similarity-search\"\n",
    "embeddings = \"OpenAI\"\n",
    "num_neighbors = 4\n",
    "retriever = make_retriever(splits, retriever_type, embeddings, num_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205c92d",
   "metadata": {},
   "source": [
    "`Make Prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bed6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. Use three sentences maximum. \n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: Think step by step \"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deb1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/31treehaus/Desktop/AI/llama.cpp/models/vicuna_13B/ggml-vicuna-13b-4bit.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  85.08 KB\n",
      "llama_model_load_internal: mem required  = 9807.48 MB (+ 1608.00 MB per state)\n",
      "............................................................warning: failed to mlock 44236800-byte buffer (after previously locking 4919476224 bytes): Resource temporarily unavailable\n",
      "........................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "def make_llm(model):\n",
    "    \"\"\"\n",
    "    Make LLM\n",
    "    @param model: LLM to use\n",
    "    @return: LLM\n",
    "    \"\"\"\n",
    "\n",
    "    if model in (\"gpt-3.5-turbo\", \"gpt-4\"):\n",
    "        llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "    elif model == \"anthropic\":\n",
    "        llm = ChatAnthropic(temperature=0)\n",
    "    elif model in (\"vicuna-7b\",\"vicuna-13b\"):\n",
    "        callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        if model == \"vicuna-7b\":\n",
    "            llm = LlamaCpp(\n",
    "                model_path=LLAMA_CPP_PATH+\"models/vicuna_7B/ggml-vicuna-7b-q4_0.bin\",\n",
    "                callback_manager=callback_manager,\n",
    "                verbose=True,\n",
    "                n_threads=6,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True)\n",
    "        else:\n",
    "            llm = LlamaCpp(\n",
    "                model_path=LLAMA_CPP_PATH+\"models/vicuna_13B/ggml-vicuna-13b-4bit.bin\",\n",
    "                callback_manager=callback_manager,\n",
    "                verbose=True,\n",
    "                n_threads=6,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True)\n",
    "    return llm\n",
    "\n",
    "llm = make_llm('vicuna-13b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48abfe",
   "metadata": {},
   "source": [
    "`Eval Set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f861a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "test_dataset = pd.read_csv(\"docs/karpathy-lex-pod/karpathy-pod-eval.csv\")\n",
    "qus = []\n",
    "for i in test_dataset.index:\n",
    "    question = test_dataset.loc[i, \"question\"]\n",
    "    answer = test_dataset.loc[i, \"answer\"]\n",
    "    data = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    qus.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e60bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is the transformer architecture expressive in the forward pass?',\n",
       " 'answer': \"The transformer architecture is expressive because it uses a general message passing scheme where nodes get to look at each other, decide what's interesting and then update each other.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003593a",
   "metadata": {},
   "source": [
    "`Run Inference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f675d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain(llm, retriever, retriever_type):\n",
    "    \"\"\"\n",
    "    Make retrieval chain\n",
    "    @param llm: model\n",
    "    @param retriever: retriever\n",
    "    @param retriever_type: retriever type\n",
    "    @return: QA chain or Llama-Index retriever, which enables QA\n",
    "    \"\"\"\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           retriever=retriever,\n",
    "                                           chain_type_kwargs=chain_type_kwargs,\n",
    "                                           input_key=\"question\")\n",
    "    return qa_chain\n",
    "\n",
    "qa_chain = make_chain(llm, retriever, retriever_type)\n",
    "result = qa_chain(qus[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469143b0",
   "metadata": {},
   "source": [
    "`Test endpoint`\n",
    "\n",
    "Deployed to `A100` on Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4372b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_8mQyorj3HycNdX7JkcraRuTQ0tjiBtH2e9gO8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ae4fa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whendid the first episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989? \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is first episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system? air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system? air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho painted the famous artwork \"The Starry Night\"?\\n\\nHard\\n\\nWhat 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho painted the famous artwork \"The Starry Night\"?\\n\\nHard\\n\\nWhat is the square root of 121? of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho painted the famous artwork \"The Starry Night\"?\\n\\nHard\\n\\nWhat is the square root of 121?  \\nWhat is the chemical symbol for hydrogen?  \\nWhat is the episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho painted the famous artwork \"The Starry Night\"?\\n\\nHard\\n\\nWhat is the square root of 121?  \\nWhat is the chemical symbol for hydrogen?  \\nWhat is the episode of \"The Sopranos\" air on HBO?  \\nWhat was the top song on the Billboard Hot 100 when the Berlin Wall fell in 1989?  \\nWhich country won the most medals at the 2016 Summer Olympics in Rio de Janeiro?\\n\\nEasy\\n\\nWhat type of fruit is typically used in a traditional smoothie?  \\nWhat is the capital of Canada?  \\nWhat is the largest planet in our solar system?  \\nWho wrote the novel \"The Great Gatsby\"?\\n\\nMedium\\n\\nWhat is the chemical symbol for gold?  \\nWhat is the tallest mammal in the world?  \\nWhat is the largest living species of lizard?  \\nWho painted the famous artwork \"The Starry Night\"?\\n\\nHard\\n\\nWhat is the square root of 121?  \\nWhat is the chemical symbol for hydrogen?  \\nWhat is the capital of France?  \\nWho painted the famous artwork \"The Mona Lisa\"?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Which NFL team won the Super Bowl when Justin Bieber was born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c4d544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "temperature was transfered to model_kwargs.\n",
      "                    Please confirm that temperature is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is the transformer architecture expressive in the forward pass?',\n",
       " 'answer': \"The transformer architecture is expressive because it uses a general message passing scheme where nodes get to look at each other, decide what's interesting and then update each other.\",\n",
       " 'result': '1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Replicate(model=\"replicate/vicuna-13b:e6d469c2b11008bb0e446c3e9629232f9674581224536851272c54871f84076e\",\n",
    "                temperature=0)\n",
    "\n",
    "qa_chain = make_chain(llm, retriever, retriever_type)\n",
    "result = qa_chain(qus[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c7b27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following pieces of context to answer the question at the end. Use three sentences maximum. \\n{context}\\nQuestion: {question}\\nAnswer: Think step by step ', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CHAIN_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2867c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "output = replicate.run(\n",
    "    \"replicate/vicuna-13b:e6d469c2b11008bb0e446c3e9629232f9674581224536851272c54871f84076e\",\n",
    "    input={\"prompt\": \"Which NFL team won the Super Bowl when Justin Bieber was born?\", \"temperature\":0.5})\n",
    "\n",
    "for message in output:\n",
    "    print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
